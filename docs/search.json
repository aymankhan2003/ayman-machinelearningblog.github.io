[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on the gradient descent algorithm\n\n\n\n\n\n\nApr 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nMar 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html",
    "href": "posts/logistic/Gradient-Descent-Blog.html",
    "title": "The Gradient Descent",
    "section": "",
    "text": "https://github.com/aymankhan2003/aymankhan2003.github.io/blob/main/posts/logistic/logistic.py"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "title": "The Gradient Descent",
    "section": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.",
    "text": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 0.1\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 5.0, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"alpha = 5.0\", color = \"red\")\n\nLR_X = LogisticRegression()\nLR_X.fit_stochastic(X, y, alpha = 100, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_X.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_X.loss_history, label = \"alpha = 100.0\", linestyle = \"--\", color = \"green\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the line with moderately higher alpha/learning rate of 5 converges better, while an alpha/learning rate of 100 does not converge as it does not fit, while an alpha/learning rate of 0.1, takes time to converge."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "title": "The Gradient Descent",
    "section": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.",
    "text": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(1, 1), (-1, -1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\", linestyle = \"--\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 30)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"batch_size = 30\", linestyle = \"-\", color = \"yellow\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the lower the batch-size is, the faster it converges, as a batch size of 30 converges faster than a batch size of 50."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "title": "The Gradient Descent",
    "section": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.",
    "text": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=False)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"no momentum\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=True)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"momentum\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThe graph clearly shows the orange line which states momentum converges faster than the blue line which indicates no momentum."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "href": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "title": "The Gradient Descent",
    "section": "All Lines Together",
    "text": "All Lines Together\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\n#Standard gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\", color = \"skyblue\")\n\n#Stochastic gradient descent\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"stochastic gradient\", color = \"purple\")\n\n#Stochastic gradient descent with momentum\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient (momentum)\", color = \"grey\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nOverall we can see the stochastic gradient with momentum to converge quicker."
  },
  {
    "objectID": "posts/perceptron/Perception Algorithm.html",
    "href": "posts/perceptron/Perception Algorithm.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Perceptron Update\n\ndef fit(self, X, y, max_steps):\n    n_samples, n_features = X.shape\n    self.w = np.random.rand(n_features+1, )\n        \n    for _ in range(self.max_steps):\n        j = np.random.randint(X.shape[0])\n        xi = np.append(X[j], 1)\n        y_hat = np.dot(xi, self.w)\n        yi = 2*y[j] - 1\n        self.w += (yi * (np.dot(xi, self.w)) < 0) * yi*xi\n                    \n        accuracy = self.score(X, y)\n        self.history.append(accuracy)\n        if self.history[_] == 1:\n            break \n\nThe major element in this update of the data plots is the fit function from the perceptron class. In order to introduce an extra bias with the value 1 to the input data, we start by randomly integrating the weights vector and adding one to n features. Our loop, where j defines a random index is taken from the number of samples in the input data “X,” is carried out with an iteration of max steps. X[j] appends 1 alongside to make the vector consistent with the weights vector. The anticipated output, denoted by the dot product of x and the weights, is denoted by y_hat, while the actual output, denoted by yi, transforms Y[j] to either 1 or -1. The weights are then updated using the loss function, and the accuracy of the perceptron point is then monitored by looking at the score history. We terminate when the history adding reaches its final iteration, which is 1, which indicates that all samples have been correctly classified.\n\n\nExample 1\n\nUsing 2d data we check if the data is linearly separable then the perceptron algorithm converges to weight vector w^tilde describing a separating line:\n\nfrom perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nExample 2\n\nUsing 2d data again, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w^tilde, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy:\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0.5, 0.5), (0.5, 0.5)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()\n\n\n\n\n\n\n\nExample 3\n\nThe perceptron algorithm is also thought to be able to work in more than 2 dimensions, we can check if it actually does:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\n\n\n\nTime Complexity of Perceptron Algorithm Update\n\nThe perceptron algorithm update has a runtime complexity of O(p), where p is the number of features. This is due to the dot product operation, which requires O(p) time, between the input vector xi and the weight vector self.w. The remaining operations are independent of the number of features and have constant time complexity."
  }
]