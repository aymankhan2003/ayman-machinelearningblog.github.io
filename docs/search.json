[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post where we use Machine Learning Models to classify the species of Palmer Penguins\n\n\n\n\n\n\nMay 23, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the notion of Unsupervised Learning with Linear Algebra\n\n\n\n\n\n\nMay 21, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the Linear Regression algorithm\n\n\n\n\n\n\nMay 2, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the works of Dr. Timnit Guru\n\n\n\n\n\n\nApr 19, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the gradient descent algorithm\n\n\n\n\n\n\nApr 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nMar 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "href": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "title": "Learning from Timnit Guru",
    "section": "",
    "text": "Dr. Gebru’s Talk\nDr. Gebru’s talk focused on the idea of how computer vision algorithms are often biased and unfair, especially when it comes to recognizing people with darker skin tones. She highlights several studies in which algorithms tend to have higher error rates for people of color which eventually lead to serious consequences such as criminal justice and hiring. Gebru emphasizes on the concept of how data is mostly trained to only include white males, where data on people of color tend to be excluded which algorithms cannot analyze hence causing a racial biasness bringing up the question of ethical compliance within many industries as they use these algorithms such as the national police force itself, as Dr. Gebru uses example oh how Batimore police use algorithms that obtains photos of Black protestors which brings a unfair case against them leading to them being arrested for no reason. Facial detection softwares are another instance of biasness which Gebru brings upon emphasis as many companies use facial detections which most black women and people of color in general tend to get mis-identified, and a huge reason for that is the implication that the data set upon the algorithm is being trained lacks the data of people of color as it is filled with a majority of white data.\ntl;dr: The number of biasness existing in Computer systems and AI is huge due to the lack of collective data in the training data and this causes unethical results in most businesses and human society.\n\n\nQuestion for Dr. Gebru\n\nWhat actions do you suggest researchers and developers take to address the existing bias issues within computer vision and AI technologies?\nHow can we ensure that the development and deployment of computer vision and AI technolgoies are more inclusive and diverse, and they prioritize the needs and well being of all individuals and communities involved?\n\n\n\nSummary of Dr.Gebru’s Talk at Middlebury\nDr. Gebru’s lecture focused on the intersections of eugenics, transhumanism, and the advancement of Artificial General Intelligence (AGI). She emphasized that the desire for AGI development stems from eugenics, which sought to improve “human stock” through genetics. TESCREAL ideologies associated with AGI envision a future in which humans merge with machines to transcend biological limitations rather than simply improving humanity.\nDr. Gebru, on the other hand, expressed reservations about who would benefit from AGI. While AGI promises a utopia, she wonders if the benefits will be limited to a select few or extend to the entire population. Large AI companies currently have the ability to develop and train AI models without adequately compensating the individuals whose data is used or the labor involved. Exploitative practices, such as the use of low-wage labor from developing countries, have already been documented.\nThe fear is that AGI will replace jobs while benefiting only a small percentage of the population, primarily AI companies. Unlike in the past, when technological advancements were eventually made available to the majority, the unique nature of AGI raises concerns that widespread benefits may not be realized. Benefit concentration must be limited because a small portion of the population cannot continue to benefit while the majority does not.\nDr. Gebru’s lecture was empowering, emphasizing the ethical concerns and potential negative consequences of AGI. She exemplified how the development of such technologies can take an ethical turn. The talk sparked further interest in the field of computer science, emphasizing the importance of future developers and researchers being aware of the potential consequences of their work.\n\n\nReflection\nOverall, Dr. Gebru presented some intriguing ideas about AI and how the intersections of eugenics and transhumanism are playing a role in creating bias among companies in the advancement of AI. Some of her statements are critical because she throws a lot based on her opinions, and her explanations aren’t always supported by logical facts. Even with these preconceptions, Dr. Gebru made significant points as she spoke and revealed biases within these large corporations that affect a large percentage of the population. She emphasizes the topic of AGI and how it only benefits a small percentage of people, which is caused by the data sets acquired by these companies, which excludes many people, resulting in bias formation. Many people may find Dr. Gebru’s talk to be a bit personal due to her use of many opinions, but she raises a lot of good points, which helps me think about social responsibility in the world of Big Data, Machine Learning, and Artificial Intelligence."
  },
  {
    "objectID": "posts/linearregression/LinearBlog.html",
    "href": "posts/linearregression/LinearBlog.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Analytical Fit Method\n\ndef fit_analytic(self, X, y):\n        X_ = self.pad(X)\n        self.w = np.linalg.inv(X_.T@X_)@X_.T@y\n\nAn analytical solution for fitting a linear regression model is implemented in the code. It accepts a feature matrix X and a target variable vector y as input. To account for the intercept term, the code adds a column of ones to the feature matrix X. It then uses the normal equation to calculate the weights of the linear regression model, yielding an analytical solution. Inverting the product of the transposed padded feature matrix, the padded feature matrix, and the target variable vector yields the weights. The weights obtained are saved in the variable self.w.\n\n\nGradient Fit Method\n\ndef fit_gradient(self, X, y, max_iter, alpha):\n        X_ = self.pad(X)\n        self.w = np.random.rand(X_.shape[1])\n        self.score_history = []\n        \n        P = X_.T@X_\n        q = X_.T@y\n        for _ in range(max_iter):\n            gradient = (P@self.w - q)\n            self.w -= alpha * gradient\n            self.score_history.append(self.score(X, y))\n\nFor fitting a linear regression model, the code uses a gradient descent algorithm. It accepts a feature matrix X and a target variable vector y as input. Using the gradient of the cost function, the algorithm iteratively updates the weights of the linear regression model. To account for the intercept term, the padded feature matrix X_ is computed by adding a column of ones to X. The weights are randomly assigned, and the algorithm iterates a set number of times. It calculates the gradient, updates the weights, and stores the training score at each iteration. The score method is used to compute the training score.\n\nThe following function will create both testing and validation data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\nGenerate data with the following code\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nUsing Linear Regression on sample data\n\nfrom linearregression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7172\nValidation score = 0.6621\n\n\n\n\nEstimated Weight Vector\n\nLR.w\n\narray([1.00952437, 0.83414312])\n\n\n\n\nGetting the same value for the weight vector using gradient descent\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_iter = 100)\nLR2.w\n\narray([1.00945598, 0.83417674])\n\n\n\n\nSeeing how the score changed over time and because we’re not using stochastic gradient descent, the score should increase monotonically in each iteration, and it does!\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nExperiment\n\nAn experiment in which p_features, the number of features used, is increased, while holding n_train, the number of training points, constant.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainingscore = []\nvalidationscore = []\n\nLR = LinearRegression()\nfor p_features in range(n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit(X_train, y_train)\n    trainingscore.append(LR.score(X_val, y_val))\n    validationscore.append(LR.score(X_train, y_train))\n\n# plot it\nplt.plot(trainingscore, label = \"Training Score\")\nplt.plot(validationscore, label = \"Validation Score\")\nplt.legend(loc='best')\nlabels = plt.gca().set(title = \"Scores vs. Features\", xlabel = \"Features\", ylabel = \"Scores\")\nplt.show()\n\n\n\n\nLooking at both scores, we can see that as both approach the score of 1.0, they start to cluster in a straight line and slowly the training score starts to decrease, indicating there could be some overfitting within the data. This means that as our model works on the training data it learns too much, while also failing to properly set upon the validation data.\n\n\n\nLASSO Experiment\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.01)\n\n\nFitting this model on some data and checking the coefficients\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.01)\n\n\n\n\nThe score on the validation set is high, which might be different from what is found with pure linear regression.\n\nL.score(X_val, y_val)\n\n0.518963781877207\n\n\n\n\nUsing LASSO\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nL1 = Lasso(alpha=0.001)\nL2 = Lasso(alpha=0.0001)\nL3 = Lasso(alpha=0.00001)\n\nLtrainingscore = []\nLvalidationscore = []\n\nL1trainingscore = []\nL1validationscore = []\n\nL2trainingscore = []\nL2validationscore = []\n\nL3trainingscore = []\nL3validationscore = []\n\nfor p_features in range(1, n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    Ltrainingscore.append(L.score(X_val, y_val))\n    Lvalidationscore.append(L.score(X_train, y_train))\n    L1.fit(X_train, y_train)\n    L1trainingscore.append(L1.score(X_val, y_val))\n    L1validationscore.append(L1.score(X_train, y_train))\n    L2.fit(X_train, y_train)\n    L2trainingscore.append(L2.score(X_val, y_val))\n    L2validationscore.append(L2.score(X_train, y_train))\n    L3.fit(X_train, y_train)\n    L3trainingscore.append(L3.score(X_val, y_val))\n    L3validationscore.append(L3.score(X_train, y_train))\n# plot it\nfig, axarr = plt.subplots(1, 4, sharex = True, sharey = True)\n\naxarr[0].plot(Ltrainingscore, label = \"training\")\naxarr[0].plot(Lvalidationscore, label = \"validation\")\naxarr[0].legend(loc='best')\nlabs = axarr[0].set(title = \"alpha = 0.01\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[1].plot(L1trainingscore, label = \"training\")\naxarr[1].plot(L1validationscore, label = \"validation\")\naxarr[1].legend(loc='best')\nlabs = axarr[1].set(title = \"alpha = 0.001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[2].plot(L2trainingscore, label = \"training\")\naxarr[2].plot(L2validationscore, label = \"validation\")\naxarr[2].legend(loc='best')\nlabs = axarr[2].set(title = \"alpha = 0.0001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[3].plot(L3trainingscore, label = \"training\")\naxarr[3].plot(L3validationscore, label = \"validation\")\naxarr[3].legend(loc='best')\nlabs = axarr[3].set(title = \"alpha = 0.00001\", xlabel = \"Features\", ylabel = \"Scores\")\n\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.010e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\nKey Takeaway: As Learning rate increases, the LASSO algrotihm declines in the way it performs as we see a shift in paradigm in comparisons of learning rates from 0.01 to 0.00001. Overall, a smaller learning rate helps to improve the data in terms of overfitting as it reduces the declination of clusters."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html",
    "href": "posts/logistic/Gradient-Descent-Blog.html",
    "title": "The Gradient Descent",
    "section": "",
    "text": "https://github.com/aymankhan2003/aymankhan2003.github.io/blob/main/posts/logistic/logistic.py"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "title": "The Gradient Descent",
    "section": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.",
    "text": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 0.1\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 5.0, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"alpha = 5.0\", color = \"red\")\n\nLR_X = LogisticRegression()\nLR_X.fit_stochastic(X, y, alpha = 100, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_X.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_X.loss_history, label = \"alpha = 100.0\", linestyle = \"--\", color = \"green\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the line with moderately higher alpha/learning rate of 5 converges better, while an alpha/learning rate of 100 does not converge as it does not fit, while an alpha/learning rate of 0.1, takes time to converge."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "title": "The Gradient Descent",
    "section": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.",
    "text": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(1, 1), (-1, -1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\", linestyle = \"--\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 30)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"batch_size = 30\", linestyle = \"-\", color = \"yellow\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the lower the batch-size is, the faster it converges, as a batch size of 30 converges faster than a batch size of 50."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "title": "The Gradient Descent",
    "section": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.",
    "text": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=False)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"no momentum\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=True)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"momentum\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThe graph clearly shows the orange line which states momentum converges faster than the blue line which indicates no momentum."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "href": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "title": "The Gradient Descent",
    "section": "All Lines Together",
    "text": "All Lines Together\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\n#Standard gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\", color = \"skyblue\")\n\n#Stochastic gradient descent\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"stochastic gradient\", color = \"purple\")\n\n#Stochastic gradient descent with momentum\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient (momentum)\", color = \"grey\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nOverall we can see the stochastic gradient with momentum to converge quicker."
  },
  {
    "objectID": "posts/penguins/Penguins-Blog.html",
    "href": "posts/penguins/Penguins-Blog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Reading the Data\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nAfter reading the data, we can see how the data looks like:\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\nVisualizing the DataSet\nThe graphs below shows the relation graph between Culmen Length and Flipper Length, classified by the Species of the Penguins and what Island they are from. From this graph we can see that Gentoo Penguins have bigger flipper lengths, than both Adelie Penguin and Chinstrap Penguins. We can also obtain that Adelie Penguins have Culmen Lengths shorter than Adelie Penguins and Chinstrap Penguins. But it is not possible to classify between Chinstrap Penguins and Adelie Penguins regarding Flipper Length, and also we cannot classify between Culmen Lengths of Gentoo Penguins and Chinstrap Penguins.\nThe relationship between Gentoo Penguins and Adelie Penguins can be classified as linearly seperable but with the addition of Chinstrap Penguins we cannot classify the relationship linearly seperable no more as the data becomes clustered. The first graph helps to also show the Island of Biscoe does not have the species of Chinstrap, while the Island of Dream does not have the species of Gentoo, and lastly the Island of Torgersen does not have both species Gentoo and Chinstrap.\nBecause of the multiple correlations between variables of each species type such as Chinstrap and Adelie regarding Flipper Lengths, and also Gentoo and Chinstrap regarding Culmen Lengths, our model would find it hard to identify Chinstrap from Adelie based on Flipper Lengths itself, and also Gentoo from Chinstrap based on Culmen Lengths.\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species', col = 'Island')\n\n<seaborn.axisgrid.FacetGrid at 0x2197a49cbe0>\n\n\n\n\n\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species')\n\n<seaborn.axisgrid.FacetGrid at 0x21979f27fa0>\n\n\n\n\n\nThe table below shows the Average Flipper Lengths, and Body Mass of the Species based on their Sex. We can see from the table that Gentoo Penguins have a greater body mass then both Chinstrap and Adelie Penguins, while Male Adelie Penguins have greater body mass then Chinstrap Penguins. The table also shows that Gentoo Penguins also have greater Flipper Lengths then the other two species while Male Adelie Penguins have greater flipper lengths then the female Chinstrap Penguins.\nUsing this table we can say that our model can classify between Gentoo Penguins from the rest but classifying the other two penguins using these two variables can be hard as they intersect in some instances. It canc lassify, but it will have to keep in track for each instance respective to it’s Sex.\n\ntrain.groupby(['Species', 'Sex'])[['Flipper Length (mm)', 'Body Mass (g)']].mean()\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      Species\n      Sex\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      187.719298\n      3337.280702\n    \n    \n      MALE\n      192.690909\n      4020.454545\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      191.551724\n      3514.655172\n    \n    \n      MALE\n      199.666667\n      3936.111111\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      FEMALE\n      212.928571\n      4677.976190\n    \n    \n      MALE\n      221.462963\n      5502.314815\n    \n  \n\n\n\n\n\n\nCleaning the DataSet\nDropping irrelavent variables that will not help with our model, and preparing our data for training:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nFeature Engineering\nThe code below takes in the given qualitative columns and quatitave columns and uses the variables to calculate the Logistic Regression scores for all combinations. Then it prints out the columns/variables with the best score, and the output below shows that the columns ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’ are the best for the model to classify data as it outputs a score very close to 1.0.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nLR = LogisticRegression()\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    \n    if score > best_score:\n        best_column = cols\n        best_score = score\n        best_model = LR\n    \nprint(best_column, best_score)\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.99609375\n\n\nWe can visualize the columns of our training data set, where 1 refers to the penguin belonging in that Island, and 0 states it doesn’t:\n\nX_train[cols]\n\n\n\n\n\n  \n    \n      \n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Flipper Length (mm)\n      Body Mass (g)\n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      215.0\n      5000.0\n    \n    \n      2\n      0\n      0\n      1\n      202.0\n      3875.0\n    \n    \n      3\n      0\n      1\n      0\n      185.0\n      3650.0\n    \n    \n      4\n      0\n      1\n      0\n      193.0\n      3800.0\n    \n    \n      5\n      0\n      1\n      0\n      178.0\n      2900.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      0\n      1\n      0\n      190.0\n      3900.0\n    \n    \n      270\n      1\n      0\n      0\n      211.0\n      4800.0\n    \n    \n      271\n      0\n      0\n      1\n      187.0\n      3150.0\n    \n    \n      272\n      1\n      0\n      0\n      224.0\n      5350.0\n    \n    \n      273\n      1\n      0\n      0\n      210.0\n      4600.0\n    \n  \n\n256 rows × 5 columns\n\n\n\nPreparing our data for future testing:\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\n\nFunction to plot our results\nThe code below creates a function that takes in the given model that we provide, and uses the testing/training datasets to display it in a nice region plot, which can help us visualize the classification properly.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n    \n      plt.title(title)\n        \n      plt.tight_layout()\n\n\n\nTesting/Training the Model\nWe can see from the result that the training dataset recieved about 97% accuracy which isn’t bad at all, showing only overfitting between three-four penguins so overall our model did a good job solely based on the training. We can see that it does not overfit between Adelie and Gentoo, but does a very little bit for Gentoo and Chinstrap which is totally fine as we are able to see a proper classification of the penguins solely on the training.\nNow we can see that the testing also obtained about 97% accuracy which also isn’t bad at all, as it does not overfit much, as it only intersects for about one penguin which is acceptable hence our model is able to classify the penguins properly achieving about 97% accuracy.\n\nX_train[\"Island_Biscoe_No\"] = X_train[[\"Island_Dream\", \"Island_Torgersen\"]].sum(axis=1)\nX_test[\"Island_Biscoe_No\"] = X_test[[\"Island_Dream\", \"Island_Torgersen\"]].sum(axis=1)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Biscoe_No']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nscore_train = LR.score(X_train[cols], y_train)\nscore_test = LR.score(X_test[cols], y_test)\n\nprint(\"Score for training: \" + str(score_train))\nplot_regions(LR, X_train[cols], y_train, title = 'Training')\n\nprint(\"Score for testing: \" + str(score_test))\nplot_regions(LR, X_test[cols], y_test, title = 'Testing')\n\nScore for training: 0.9765625\nScore for testing: 0.9705882352941176"
  },
  {
    "objectID": "posts/perceptron/Perception Algorithm.html",
    "href": "posts/perceptron/Perception Algorithm.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Perceptron Update\n\ndef fit(self, X, y, max_steps):\n    n_samples, n_features = X.shape\n    self.w = np.random.rand(n_features+1, )\n        \n    for _ in range(self.max_steps):\n        j = np.random.randint(X.shape[0])\n        xi = np.append(X[j], 1)\n        y_hat = np.dot(xi, self.w)\n        yi = 2*y[j] - 1\n        self.w += (yi * (np.dot(xi, self.w)) < 0) * yi*xi\n                    \n        accuracy = self.score(X, y)\n        self.history.append(accuracy)\n        if self.history[_] == 1:\n            break \n\nThe major element in this update of the data plots is the fit function from the perceptron class. In order to introduce an extra bias with the value 1 to the input data, we start by randomly integrating the weights vector and adding one to n features. Our loop, where j defines a random index is taken from the number of samples in the input data “X,” is carried out with an iteration of max steps. X[j] appends 1 alongside to make the vector consistent with the weights vector. The anticipated output, denoted by the dot product of x and the weights, is denoted by y_hat, while the actual output, denoted by yi, transforms Y[j] to either 1 or -1. The weights are then updated using the loss function, and the accuracy of the perceptron point is then monitored by looking at the score history. We terminate when the history adding reaches its final iteration, which is 1, which indicates that all samples have been correctly classified.\n\n\nExample 1\n\nUsing 2d data we check if the data is linearly separable then the perceptron algorithm converges to weight vector w^tilde describing a separating line:\n\nfrom perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nExample 2\n\nUsing 2d data again, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w^tilde, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy:\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0.5, 0.5), (0.5, 0.5)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()\n\n\n\n\n\n\n\nExample 3\n\nThe perceptron algorithm is also thought to be able to work in more than 2 dimensions, we can check if it actually does:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\n\n\n\nTime Complexity of Perceptron Algorithm Update\n\nThe perceptron algorithm update has a runtime complexity of O(p), where p is the number of features. This is due to the dot product operation, which requires O(p) time, between the input vector xi and the weight vector self.w. The remaining operations are independent of the number of features and have constant time complexity."
  },
  {
    "objectID": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "href": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\n\na_1 = np.random.randint(1, 3, (5, 3))\na_2 = np.random.randint(1, 3, (3, 7))\n\nA = a_1 @ a_2 + 0.1*np.random.randn(5, 7)\nA\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\n\n\n\nplt.imshow(A, cmap = \"Greys\")\na = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nU, sigma, V = np.linalg.svd(A)\n\n\n# create the D matrix in the SVD\nD = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\nD[:min(A.shape),:min(A.shape)] = np.diag(sigma)        # singular values on the main diagonal\nD\n\narray([[47.92787806,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  1.59525638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.35772778,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.23426796,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.15684841,\n         0.        ,  0.        ]])\n\n\n\nU @ D @ V # == A up to numerical precision\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\nk = 2\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\n\n\nA_ = U_ @ D_ @ V_\n\n\n\n\n\ndef compare_images(A, A_):\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n\ncompare_images(A, A_)\n\n\n\n\n\nk = 1\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\nA_ = U_ @ D_ @ V_\ncompare_images(A, A_)\n\n\n\n\nThe images almost look visually close and similar\n\n\n\n\n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://wallpapers.com/images/hd/cute-spongebob-playing-soccer-cd7tg9yci4brk87w.jpg\"\n\nimg = read_image(url)\n\n\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\ngrey_img.shape\n\n(1080, 1920)\n\n\n\n\n\n\ndef svd_reconstruct(image, k):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    A = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A\n\n\n\n\n\n\n\n\n\n\ndef svd_experiment(image):\n    # Original image dimensions\n    m, n = image.shape\n    \n    # Total number of pixels in the original image\n    total_pixels = m * n\n    \n    rows = 4\n    cols = 4\n    fig, axarr = plt.subplots(rows, cols, figsize=(20, 10))\n    \n    k_values = [5 * i for i in range(1, 17)]\n    \n    for i, k in enumerate(k_values):\n        reconstructed_image = svd_reconstruct(image, k)\n        \n        # Calculate the storage required for the reconstructed image\n        reconstructed_pixels = k * (m + n + k) \n            \n        # Display the reconstructed image for visual comparison\n        subplot_row = i // cols\n        subplot_col = i % cols\n        subplot_ax = axarr[subplot_row, subplot_col]\n        \n        subplot_ax.imshow(reconstructed_image, cmap='Greys')\n        title = f'{k} components %storage = {round((reconstructed_pixels / total_pixels)*100, 1)}'\n        subplot_ax.set_title(title)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nsvd_experiment(grey_img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef svd_reconstruct(image, cf):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    m, n = image.shape\n    #Solving compression factor equation in terms of k\n    k = (-m + n + np.sqrt(m**2 + 2*m*n + n**2 + 4*cf*m*n)/2)\n    k = np.floor(k)\n    k = k.astype(int)\n    A_ = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A_\n\n\nnewimage = svd_reconstruct(grey_img, 20)\ncompare_images(grey_img, newimage)"
  }
]