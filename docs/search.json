[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on the notion of Unsupervised Learning with Linear Algebra\n\n\n\n\n\n\nMay 21, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the Linear Regression algorithm\n\n\n\n\n\n\nMay 2, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the works of Dr. Timnit Guru\n\n\n\n\n\n\nApr 19, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the gradient descent algorithm\n\n\n\n\n\n\nApr 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nMar 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "href": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "title": "Learning from Timnit Guru",
    "section": "",
    "text": "Dr. Gebru’s Talk\nDr. Gebru’s talk focused on the idea of how computer vision algorithms are often biased and unfair, especially when it comes to recognizing people with darker skin tones. She highlights several studies in which algorithms tend to have higher error rates for people of color which eventually lead to serious consequences such as criminal justice and hiring. Gebru emphasizes on the concept of how data is mostly trained to only include white males, where data on people of color tend to be excluded which algorithms cannot analyze hence causing a racial biasness bringing up the question of ethical compliance within many industries as they use these algorithms such as the national police force itself, as Dr. Gebru uses example oh how Batimore police use algorithms that obtains photos of Black protestors which brings a unfair case against them leading to them being arrested for no reason. Facial detection softwares are another instance of biasness which Gebru brings upon emphasis as many companies use facial detections which most black women and people of color in general tend to get mis-identified, and a huge reason for that is the implication that the data set upon the algorithm is being trained lacks the data of people of color as it is filled with a majority of white data.\ntl;dr: The number of biasness existing in Computer systems and AI is huge due to the lack of collective data in the training data and this causes unethical results in most businesses and human society.\n\n\nQuestion for Dr. Gebru\n\nWhat actions do you suggest researchers and developers take to address the existing bias issues within computer vision and AI technologies?\nHow can we ensure that the development and deployment of computer vision and AI technolgoies are more inclusive and diverse, and they prioritize the needs and well being of all individuals and communities involved?\n\n\n\nSummary of Dr.Gebru’s Talk at Middlebury\nDr. Gebru’s lecture focused on the intersections of eugenics, transhumanism, and the advancement of Artificial General Intelligence (AGI). She emphasized that the desire for AGI development stems from eugenics, which sought to improve “human stock” through genetics. TESCREAL ideologies associated with AGI envision a future in which humans merge with machines to transcend biological limitations rather than simply improving humanity.\nDr. Gebru, on the other hand, expressed reservations about who would benefit from AGI. While AGI promises a utopia, she wonders if the benefits will be limited to a select few or extend to the entire population. Large AI companies currently have the ability to develop and train AI models without adequately compensating the individuals whose data is used or the labor involved. Exploitative practices, such as the use of low-wage labor from developing countries, have already been documented.\nThe fear is that AGI will replace jobs while benefiting only a small percentage of the population, primarily AI companies. Unlike in the past, when technological advancements were eventually made available to the majority, the unique nature of AGI raises concerns that widespread benefits may not be realized. Benefit concentration must be limited because a small portion of the population cannot continue to benefit while the majority does not.\nDr. Gebru’s lecture was empowering, emphasizing the ethical concerns and potential negative consequences of AGI. She exemplified how the development of such technologies can take an ethical turn. The talk sparked further interest in the field of computer science, emphasizing the importance of future developers and researchers being aware of the potential consequences of their work.\n\n\nReflection\nOverall, Dr. Gebru presented some intriguing ideas about AI and how the intersections of eugenics and transhumanism are playing a role in creating bias among companies in the advancement of AI. Some of her statements are critical because she throws a lot based on her opinions, and her explanations aren’t always supported by logical facts. Even with these preconceptions, Dr. Gebru made significant points as she spoke and revealed biases within these large corporations that affect a large percentage of the population. She emphasizes the topic of AGI and how it only benefits a small percentage of people, which is caused by the data sets acquired by these companies, which excludes many people, resulting in bias formation. Many people may find Dr. Gebru’s talk to be a bit personal due to her use of many opinions, but she raises a lot of good points, which helps me think about social responsibility in the world of Big Data, Machine Learning, and Artificial Intelligence."
  },
  {
    "objectID": "posts/linearregression/LinearBlog.html",
    "href": "posts/linearregression/LinearBlog.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Analytical Fit Method\n\ndef fit_analytic(self, X, y):\n        X_ = self.pad(X)\n        self.w = np.linalg.inv(X_.T@X_)@X_.T@y\n\nAn analytical solution for fitting a linear regression model is implemented in the code. It accepts a feature matrix X and a target variable vector y as input. To account for the intercept term, the code adds a column of ones to the feature matrix X. It then uses the normal equation to calculate the weights of the linear regression model, yielding an analytical solution. Inverting the product of the transposed padded feature matrix, the padded feature matrix, and the target variable vector yields the weights. The weights obtained are saved in the variable self.w.\n\n\nGradient Fit Method\n\ndef fit_gradient(self, X, y, max_iter, alpha):\n        X_ = self.pad(X)\n        self.w = np.random.rand(X_.shape[1])\n        self.score_history = []\n        \n        P = X_.T@X_\n        q = X_.T@y\n        for _ in range(max_iter):\n            gradient = (P@self.w - q)\n            self.w -= alpha * gradient\n            self.score_history.append(self.score(X, y))\n\nFor fitting a linear regression model, the code uses a gradient descent algorithm. It accepts a feature matrix X and a target variable vector y as input. Using the gradient of the cost function, the algorithm iteratively updates the weights of the linear regression model. To account for the intercept term, the padded feature matrix X_ is computed by adding a column of ones to X. The weights are randomly assigned, and the algorithm iterates a set number of times. It calculates the gradient, updates the weights, and stores the training score at each iteration. The score method is used to compute the training score.\n\nThe following function will create both testing and validation data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\nGenerate data with the following code\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nUsing Linear Regression on sample data\n\nfrom linearregression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7172\nValidation score = 0.6621\n\n\n\n\nEstimated Weight Vector\n\nLR.w\n\narray([1.00952437, 0.83414312])\n\n\n\n\nGetting the same value for the weight vector using gradient descent\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_iter = 100)\nLR2.w\n\narray([1.00945598, 0.83417674])\n\n\n\n\nSeeing how the score changed over time and because we’re not using stochastic gradient descent, the score should increase monotonically in each iteration, and it does!\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nExperiment\n\nAn experiment in which p_features, the number of features used, is increased, while holding n_train, the number of training points, constant.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainingscore = []\nvalidationscore = []\n\nLR = LinearRegression()\nfor p_features in range(n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit(X_train, y_train)\n    trainingscore.append(LR.score(X_val, y_val))\n    validationscore.append(LR.score(X_train, y_train))\n\n# plot it\nplt.plot(trainingscore, label = \"Training Score\")\nplt.plot(validationscore, label = \"Validation Score\")\nplt.legend(loc='best')\nlabels = plt.gca().set(title = \"Scores vs. Features\", xlabel = \"Features\", ylabel = \"Scores\")\nplt.show()\n\n\n\n\nLooking at both scores, we can see that as both approach the score of 1.0, they start to cluster in a straight line and slowly the training score starts to decrease, indicating there could be some overfitting within the data. This means that as our model works on the training data it learns too much, while also failing to properly set upon the validation data.\n\n\n\nLASSO Experiment\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.01)\n\n\nFitting this model on some data and checking the coefficients\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.01)\n\n\n\n\nThe score on the validation set is high, which might be different from what is found with pure linear regression.\n\nL.score(X_val, y_val)\n\n0.518963781877207\n\n\n\n\nUsing LASSO\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nL1 = Lasso(alpha=0.001)\nL2 = Lasso(alpha=0.0001)\nL3 = Lasso(alpha=0.00001)\n\nLtrainingscore = []\nLvalidationscore = []\n\nL1trainingscore = []\nL1validationscore = []\n\nL2trainingscore = []\nL2validationscore = []\n\nL3trainingscore = []\nL3validationscore = []\n\nfor p_features in range(1, n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    Ltrainingscore.append(L.score(X_val, y_val))\n    Lvalidationscore.append(L.score(X_train, y_train))\n    L1.fit(X_train, y_train)\n    L1trainingscore.append(L1.score(X_val, y_val))\n    L1validationscore.append(L1.score(X_train, y_train))\n    L2.fit(X_train, y_train)\n    L2trainingscore.append(L2.score(X_val, y_val))\n    L2validationscore.append(L2.score(X_train, y_train))\n    L3.fit(X_train, y_train)\n    L3trainingscore.append(L3.score(X_val, y_val))\n    L3validationscore.append(L3.score(X_train, y_train))\n# plot it\nfig, axarr = plt.subplots(1, 4, sharex = True, sharey = True)\n\naxarr[0].plot(Ltrainingscore, label = \"training\")\naxarr[0].plot(Lvalidationscore, label = \"validation\")\naxarr[0].legend(loc='best')\nlabs = axarr[0].set(title = \"alpha = 0.01\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[1].plot(L1trainingscore, label = \"training\")\naxarr[1].plot(L1validationscore, label = \"validation\")\naxarr[1].legend(loc='best')\nlabs = axarr[1].set(title = \"alpha = 0.001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[2].plot(L2trainingscore, label = \"training\")\naxarr[2].plot(L2validationscore, label = \"validation\")\naxarr[2].legend(loc='best')\nlabs = axarr[2].set(title = \"alpha = 0.0001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[3].plot(L3trainingscore, label = \"training\")\naxarr[3].plot(L3validationscore, label = \"validation\")\naxarr[3].legend(loc='best')\nlabs = axarr[3].set(title = \"alpha = 0.00001\", xlabel = \"Features\", ylabel = \"Scores\")\n\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.010e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\nKey Takeaway: As Learning rate increases, the LASSO algrotihm declines in the way it performs as we see a shift in paradigm in comparisons of learning rates from 0.01 to 0.00001. Overall, a smaller learning rate helps to improve the data in terms of overfitting as it reduces the declination of clusters."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html",
    "href": "posts/logistic/Gradient-Descent-Blog.html",
    "title": "The Gradient Descent",
    "section": "",
    "text": "https://github.com/aymankhan2003/aymankhan2003.github.io/blob/main/posts/logistic/logistic.py"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "title": "The Gradient Descent",
    "section": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.",
    "text": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 0.1\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 5.0, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"alpha = 5.0\", color = \"red\")\n\nLR_X = LogisticRegression()\nLR_X.fit_stochastic(X, y, alpha = 100, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_X.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_X.loss_history, label = \"alpha = 100.0\", linestyle = \"--\", color = \"green\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the line with moderately higher alpha/learning rate of 5 converges better, while an alpha/learning rate of 100 does not converge as it does not fit, while an alpha/learning rate of 0.1, takes time to converge."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "title": "The Gradient Descent",
    "section": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.",
    "text": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(1, 1), (-1, -1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\", linestyle = \"--\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 30)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"batch_size = 30\", linestyle = \"-\", color = \"yellow\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the lower the batch-size is, the faster it converges, as a batch size of 30 converges faster than a batch size of 50."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "title": "The Gradient Descent",
    "section": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.",
    "text": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=False)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"no momentum\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=True)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"momentum\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThe graph clearly shows the orange line which states momentum converges faster than the blue line which indicates no momentum."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "href": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "title": "The Gradient Descent",
    "section": "All Lines Together",
    "text": "All Lines Together\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\n#Standard gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\", color = \"skyblue\")\n\n#Stochastic gradient descent\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"stochastic gradient\", color = \"purple\")\n\n#Stochastic gradient descent with momentum\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient (momentum)\", color = \"grey\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nOverall we can see the stochastic gradient with momentum to converge quicker."
  },
  {
    "objectID": "posts/perceptron/Perception Algorithm.html",
    "href": "posts/perceptron/Perception Algorithm.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Perceptron Update\n\ndef fit(self, X, y, max_steps):\n    n_samples, n_features = X.shape\n    self.w = np.random.rand(n_features+1, )\n        \n    for _ in range(self.max_steps):\n        j = np.random.randint(X.shape[0])\n        xi = np.append(X[j], 1)\n        y_hat = np.dot(xi, self.w)\n        yi = 2*y[j] - 1\n        self.w += (yi * (np.dot(xi, self.w)) < 0) * yi*xi\n                    \n        accuracy = self.score(X, y)\n        self.history.append(accuracy)\n        if self.history[_] == 1:\n            break \n\nThe major element in this update of the data plots is the fit function from the perceptron class. In order to introduce an extra bias with the value 1 to the input data, we start by randomly integrating the weights vector and adding one to n features. Our loop, where j defines a random index is taken from the number of samples in the input data “X,” is carried out with an iteration of max steps. X[j] appends 1 alongside to make the vector consistent with the weights vector. The anticipated output, denoted by the dot product of x and the weights, is denoted by y_hat, while the actual output, denoted by yi, transforms Y[j] to either 1 or -1. The weights are then updated using the loss function, and the accuracy of the perceptron point is then monitored by looking at the score history. We terminate when the history adding reaches its final iteration, which is 1, which indicates that all samples have been correctly classified.\n\n\nExample 1\n\nUsing 2d data we check if the data is linearly separable then the perceptron algorithm converges to weight vector w^tilde describing a separating line:\n\nfrom perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nExample 2\n\nUsing 2d data again, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w^tilde, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy:\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0.5, 0.5), (0.5, 0.5)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()\n\n\n\n\n\n\n\nExample 3\n\nThe perceptron algorithm is also thought to be able to work in more than 2 dimensions, we can check if it actually does:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\n\n\n\nTime Complexity of Perceptron Algorithm Update\n\nThe perceptron algorithm update has a runtime complexity of O(p), where p is the number of features. This is due to the dot product operation, which requires O(p) time, between the input vector xi and the weight vector self.w. The remaining operations are independent of the number of features and have constant time complexity."
  },
  {
    "objectID": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "href": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\n\na_1 = np.random.randint(1, 3, (5, 3))\na_2 = np.random.randint(1, 3, (3, 7))\n\nA = a_1 @ a_2 + 0.1*np.random.randn(5, 7)\nA\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\n\n\n\nplt.imshow(A, cmap = \"Greys\")\na = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nU, sigma, V = np.linalg.svd(A)\n\n\n# create the D matrix in the SVD\nD = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\nD[:min(A.shape),:min(A.shape)] = np.diag(sigma)        # singular values on the main diagonal\nD\n\narray([[47.92787806,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  1.59525638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.35772778,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.23426796,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.15684841,\n         0.        ,  0.        ]])\n\n\n\nU @ D @ V # == A up to numerical precision\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\nk = 2\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\n\n\nA_ = U_ @ D_ @ V_\n\n\n\n\n\ndef compare_images(A, A_):\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n\ncompare_images(A, A_)\n\n\n\n\n\nk = 1\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\nA_ = U_ @ D_ @ V_\ncompare_images(A, A_)\n\n\n\n\nThe images almost look visually close and similar\n\n\n\n\n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://wallpapers.com/images/hd/cute-spongebob-playing-soccer-cd7tg9yci4brk87w.jpg\"\n\nimg = read_image(url)\n\n\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\ngrey_img.shape\n\n(1080, 1920)\n\n\n\n\n\n\ndef svd_reconstruct(image, k):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    A = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A\n\n\n\n\n\n\n\n\n\n\ndef svd_experiment(image):\n    # Original image dimensions\n    m, n = image.shape\n    \n    # Total number of pixels in the original image\n    total_pixels = m * n\n    \n    rows = 4\n    cols = 4\n    fig, axarr = plt.subplots(rows, cols, figsize=(20, 10))\n    \n    k_values = [5 * i for i in range(1, 17)]\n    \n    for i, k in enumerate(k_values):\n        reconstructed_image = svd_reconstruct(image, k)\n        \n        # Calculate the storage required for the reconstructed image\n        reconstructed_pixels = k * (m + n + k) \n            \n        # Display the reconstructed image for visual comparison\n        subplot_row = i // cols\n        subplot_col = i % cols\n        subplot_ax = axarr[subplot_row, subplot_col]\n        \n        subplot_ax.imshow(reconstructed_image, cmap='Greys')\n        title = f'{k} components %storage = {round((reconstructed_pixels / total_pixels)*100, 1)}'\n        subplot_ax.set_title(title)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nsvd_experiment(grey_img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef svd_reconstruct(image, cf):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    m, n = image.shape\n    #Solving compression factor equation in terms of k\n    k = (-m + n + np.sqrt(m**2 + 2*m*n + n**2 + 4*cf*m*n)/2)\n    k = np.floor(k)\n    k = k.astype(int)\n    A_ = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A_\n\n\nnewimage = svd_reconstruct(grey_img, 20)\ncompare_images(grey_img, newimage)"
  }
]