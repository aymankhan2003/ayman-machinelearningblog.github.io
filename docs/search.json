[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on the works of Dr. Timnit Guru\n\n\n\n\n\n\nApr 19, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the gradient descent algorithm\n\n\n\n\n\n\nApr 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nMar 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "href": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "title": "Learning from Timnit Guru",
    "section": "",
    "text": "Dr. Gebru’s Talk\nDr. Gebru’s talk focused on the idea of how computer vision algorithms are often biased and unfair, especially when it comes to recognizing people with darker skin tones. She highlights several studies in which algorithms tend to have higher error rates for people of color which eventually lead to serious consequences such as criminal justice and hiring. Gebru emphasizes on the concept of how data is mostly trained to only include white males, where data on people of color tend to be excluded which algorithms cannot analyze hence causing a racial biasness bringing up the question of ethical compliance within many industries as they use these algorithms such as the national police force itself, as Dr. Gebru uses example oh how Batimore police use algorithms that obtains photos of Black protestors which brings a unfair case against them leading to them being arrested for no reason. Facial detection softwares are another instance of biasness which Gebru brings upon emphasis as many companies use facial detections which most black women and people of color in general tend to get mis-identified, and a huge reason for that is the implication that the data set upon the algorithm is being trained lacks the data of people of color as it is filled with a majority of white data.\ntl;dr: The number of biasness existing in Computer systems and AI is huge due to the lack of collective data in the training data and this causes unethical results in most businesses and human society.\n\n\nQuestion for Dr. Gebru\n\nWhat actions do you suggest researchers and developers take to address the existing bias issues within computer vision and AI technologies?\nHow can we ensure that the development and deployment of computer vision and AI technolgoies are more inclusive and diverse, and they prioritize the needs and well being of all individuals and communities involved?"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html",
    "href": "posts/logistic/Gradient-Descent-Blog.html",
    "title": "The Gradient Descent",
    "section": "",
    "text": "https://github.com/aymankhan2003/aymankhan2003.github.io/blob/main/posts/logistic/logistic.py"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "title": "The Gradient Descent",
    "section": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.",
    "text": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 0.1\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 5.0, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"alpha = 5.0\", color = \"red\")\n\nLR_X = LogisticRegression()\nLR_X.fit_stochastic(X, y, alpha = 100, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_X.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_X.loss_history, label = \"alpha = 100.0\", linestyle = \"--\", color = \"green\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the line with moderately higher alpha/learning rate of 5 converges better, while an alpha/learning rate of 100 does not converge as it does not fit, while an alpha/learning rate of 0.1, takes time to converge."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "title": "The Gradient Descent",
    "section": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.",
    "text": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(1, 1), (-1, -1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\", linestyle = \"--\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 30)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"batch_size = 30\", linestyle = \"-\", color = \"yellow\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the lower the batch-size is, the faster it converges, as a batch size of 30 converges faster than a batch size of 50."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "title": "The Gradient Descent",
    "section": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.",
    "text": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=False)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"no momentum\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=True)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"momentum\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThe graph clearly shows the orange line which states momentum converges faster than the blue line which indicates no momentum."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "href": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "title": "The Gradient Descent",
    "section": "All Lines Together",
    "text": "All Lines Together\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\n#Standard gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\", color = \"skyblue\")\n\n#Stochastic gradient descent\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"stochastic gradient\", color = \"purple\")\n\n#Stochastic gradient descent with momentum\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient (momentum)\", color = \"grey\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nOverall we can see the stochastic gradient with momentum to converge quicker."
  },
  {
    "objectID": "posts/perceptron/Perception Algorithm.html",
    "href": "posts/perceptron/Perception Algorithm.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Perceptron Update\n\ndef fit(self, X, y, max_steps):\n    n_samples, n_features = X.shape\n    self.w = np.random.rand(n_features+1, )\n        \n    for _ in range(self.max_steps):\n        j = np.random.randint(X.shape[0])\n        xi = np.append(X[j], 1)\n        y_hat = np.dot(xi, self.w)\n        yi = 2*y[j] - 1\n        self.w += (yi * (np.dot(xi, self.w)) < 0) * yi*xi\n                    \n        accuracy = self.score(X, y)\n        self.history.append(accuracy)\n        if self.history[_] == 1:\n            break \n\nThe major element in this update of the data plots is the fit function from the perceptron class. In order to introduce an extra bias with the value 1 to the input data, we start by randomly integrating the weights vector and adding one to n features. Our loop, where j defines a random index is taken from the number of samples in the input data “X,” is carried out with an iteration of max steps. X[j] appends 1 alongside to make the vector consistent with the weights vector. The anticipated output, denoted by the dot product of x and the weights, is denoted by y_hat, while the actual output, denoted by yi, transforms Y[j] to either 1 or -1. The weights are then updated using the loss function, and the accuracy of the perceptron point is then monitored by looking at the score history. We terminate when the history adding reaches its final iteration, which is 1, which indicates that all samples have been correctly classified.\n\n\nExample 1\n\nUsing 2d data we check if the data is linearly separable then the perceptron algorithm converges to weight vector w^tilde describing a separating line:\n\nfrom perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nExample 2\n\nUsing 2d data again, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w^tilde, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy:\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0.5, 0.5), (0.5, 0.5)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()\n\n\n\n\n\n\n\nExample 3\n\nThe perceptron algorithm is also thought to be able to work in more than 2 dimensions, we can check if it actually does:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\n\n\n\nTime Complexity of Perceptron Algorithm Update\n\nThe perceptron algorithm update has a runtime complexity of O(p), where p is the number of features. This is due to the dot product operation, which requires O(p) time, between the input vector xi and the weight vector self.w. The remaining operations are independent of the number of features and have constant time complexity."
  }
]