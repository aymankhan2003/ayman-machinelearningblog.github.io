[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post where we use Machine Learning Models to classify the species of Palmer Penguins\n\n\n\n\n\n\nMay 23, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post where we try to predict market values and trasnfer fees of soccer players.\n\n\n\n\n\n\nMay 23, 2023\n\n\nAyman Khan, Hedavam Solano, Anweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the notion of Unsupervised Learning with Linear Algebra\n\n\n\n\n\n\nMay 21, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the Linear Regression algorithm\n\n\n\n\n\n\nMay 2, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the works of Dr. Timnit Guru\n\n\n\n\n\n\nApr 19, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the gradient descent algorithm\n\n\n\n\n\n\nApr 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nMar 11, 2023\n\n\nAyman Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "href": "posts/Dr.TimnitGebru/Dr.TimnitGebru.html",
    "title": "Learning from Timnit Guru",
    "section": "",
    "text": "Dr. Gebru’s Talk\nDr. Gebru’s talk focused on the idea of how computer vision algorithms are often biased and unfair, especially when it comes to recognizing people with darker skin tones. She highlights several studies in which algorithms tend to have higher error rates for people of color which eventually lead to serious consequences such as criminal justice and hiring. Gebru emphasizes on the concept of how data is mostly trained to only include white males, where data on people of color tend to be excluded which algorithms cannot analyze hence causing a racial biasness bringing up the question of ethical compliance within many industries as they use these algorithms such as the national police force itself, as Dr. Gebru uses example oh how Batimore police use algorithms that obtains photos of Black protestors which brings a unfair case against them leading to them being arrested for no reason. Facial detection softwares are another instance of biasness which Gebru brings upon emphasis as many companies use facial detections which most black women and people of color in general tend to get mis-identified, and a huge reason for that is the implication that the data set upon the algorithm is being trained lacks the data of people of color as it is filled with a majority of white data.\ntl;dr: The number of biasness existing in Computer systems and AI is huge due to the lack of collective data in the training data and this causes unethical results in most businesses and human society.\n\n\nQuestion for Dr. Gebru\n\nWhat actions do you suggest researchers and developers take to address the existing bias issues within computer vision and AI technologies?\nHow can we ensure that the development and deployment of computer vision and AI technolgoies are more inclusive and diverse, and they prioritize the needs and well being of all individuals and communities involved?\n\n\n\nSummary of Dr.Gebru’s Talk at Middlebury\nDr. Gebru’s lecture focused on the intersections of eugenics, transhumanism, and the advancement of Artificial General Intelligence (AGI). She emphasized that the desire for AGI development stems from eugenics, which sought to improve “human stock” through genetics. TESCREAL ideologies associated with AGI envision a future in which humans merge with machines to transcend biological limitations rather than simply improving humanity.\nDr. Gebru, on the other hand, expressed reservations about who would benefit from AGI. While AGI promises a utopia, she wonders if the benefits will be limited to a select few or extend to the entire population. Large AI companies currently have the ability to develop and train AI models without adequately compensating the individuals whose data is used or the labor involved. Exploitative practices, such as the use of low-wage labor from developing countries, have already been documented.\nThe fear is that AGI will replace jobs while benefiting only a small percentage of the population, primarily AI companies. Unlike in the past, when technological advancements were eventually made available to the majority, the unique nature of AGI raises concerns that widespread benefits may not be realized. Benefit concentration must be limited because a small portion of the population cannot continue to benefit while the majority does not.\nDr. Gebru’s lecture was empowering, emphasizing the ethical concerns and potential negative consequences of AGI. She exemplified how the development of such technologies can take an ethical turn. The talk sparked further interest in the field of computer science, emphasizing the importance of future developers and researchers being aware of the potential consequences of their work.\n\n\nReflection\nOverall, Dr. Gebru presented some intriguing ideas about AI and how the intersections of eugenics and transhumanism are playing a role in creating bias among companies in the advancement of AI. Some of her statements are critical because she throws a lot based on her opinions, and her explanations aren’t always supported by logical facts. Even with these preconceptions, Dr. Gebru made significant points as she spoke and revealed biases within these large corporations that affect a large percentage of the population. She emphasizes the topic of AGI and how it only benefits a small percentage of people, which is caused by the data sets acquired by these companies, which excludes many people, resulting in bias formation. Many people may find Dr. Gebru’s talk to be a bit personal due to her use of many opinions, but she raises a lot of good points, which helps me think about social responsibility in the world of Big Data, Machine Learning, and Artificial Intelligence."
  },
  {
    "objectID": "posts/linearregression/LinearBlog.html",
    "href": "posts/linearregression/LinearBlog.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Analytical Fit Method\n\ndef fit_analytic(self, X, y):\n        X_ = self.pad(X)\n        self.w = np.linalg.inv(X_.T@X_)@X_.T@y\n\nAn analytical solution for fitting a linear regression model is implemented in the code. It accepts a feature matrix X and a target variable vector y as input. To account for the intercept term, the code adds a column of ones to the feature matrix X. It then uses the normal equation to calculate the weights of the linear regression model, yielding an analytical solution. Inverting the product of the transposed padded feature matrix, the padded feature matrix, and the target variable vector yields the weights. The weights obtained are saved in the variable self.w.\n\n\nGradient Fit Method\n\ndef fit_gradient(self, X, y, max_iter, alpha):\n        X_ = self.pad(X)\n        self.w = np.random.rand(X_.shape[1])\n        self.score_history = []\n        \n        P = X_.T@X_\n        q = X_.T@y\n        for _ in range(max_iter):\n            gradient = (P@self.w - q)\n            self.w -= alpha * gradient\n            self.score_history.append(self.score(X, y))\n\nFor fitting a linear regression model, the code uses a gradient descent algorithm. It accepts a feature matrix X and a target variable vector y as input. Using the gradient of the cost function, the algorithm iteratively updates the weights of the linear regression model. To account for the intercept term, the padded feature matrix X_ is computed by adding a column of ones to X. The weights are randomly assigned, and the algorithm iterates a set number of times. It calculates the gradient, updates the weights, and stores the training score at each iteration. The score method is used to compute the training score.\n\nThe following function will create both testing and validation data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\nGenerate data with the following code\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nUsing Linear Regression on sample data\n\nfrom linearregression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7172\nValidation score = 0.6621\n\n\n\n\nEstimated Weight Vector\n\nLR.w\n\narray([1.00952437, 0.83414312])\n\n\n\n\nGetting the same value for the weight vector using gradient descent\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_iter = 100)\nLR2.w\n\narray([1.00945598, 0.83417674])\n\n\n\n\nSeeing how the score changed over time and because we’re not using stochastic gradient descent, the score should increase monotonically in each iteration, and it does!\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nExperiment\n\nAn experiment in which p_features, the number of features used, is increased, while holding n_train, the number of training points, constant.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainingscore = []\nvalidationscore = []\n\nLR = LinearRegression()\nfor p_features in range(n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit(X_train, y_train)\n    trainingscore.append(LR.score(X_val, y_val))\n    validationscore.append(LR.score(X_train, y_train))\n\n# plot it\nplt.plot(trainingscore, label = \"Training Score\")\nplt.plot(validationscore, label = \"Validation Score\")\nplt.legend(loc='best')\nlabels = plt.gca().set(title = \"Scores vs. Features\", xlabel = \"Features\", ylabel = \"Scores\")\nplt.show()\n\n\n\n\nLooking at both scores, we can see that as both approach the score of 1.0, they start to cluster in a straight line and slowly the training score starts to decrease, indicating there could be some overfitting within the data. This means that as our model works on the training data it learns too much, while also failing to properly set upon the validation data.\n\n\n\nLASSO Experiment\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.01)\n\n\nFitting this model on some data and checking the coefficients\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.01)\n\n\n\n\nThe score on the validation set is high, which might be different from what is found with pure linear regression.\n\nL.score(X_val, y_val)\n\n0.518963781877207\n\n\n\n\nUsing LASSO\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nL1 = Lasso(alpha=0.001)\nL2 = Lasso(alpha=0.0001)\nL3 = Lasso(alpha=0.00001)\n\nLtrainingscore = []\nLvalidationscore = []\n\nL1trainingscore = []\nL1validationscore = []\n\nL2trainingscore = []\nL2validationscore = []\n\nL3trainingscore = []\nL3validationscore = []\n\nfor p_features in range(1, n_train - 1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    Ltrainingscore.append(L.score(X_val, y_val))\n    Lvalidationscore.append(L.score(X_train, y_train))\n    L1.fit(X_train, y_train)\n    L1trainingscore.append(L1.score(X_val, y_val))\n    L1validationscore.append(L1.score(X_train, y_train))\n    L2.fit(X_train, y_train)\n    L2trainingscore.append(L2.score(X_val, y_val))\n    L2validationscore.append(L2.score(X_train, y_train))\n    L3.fit(X_train, y_train)\n    L3trainingscore.append(L3.score(X_val, y_val))\n    L3validationscore.append(L3.score(X_train, y_train))\n# plot it\nfig, axarr = plt.subplots(1, 4, sharex = True, sharey = True)\n\naxarr[0].plot(Ltrainingscore, label = \"training\")\naxarr[0].plot(Lvalidationscore, label = \"validation\")\naxarr[0].legend(loc='best')\nlabs = axarr[0].set(title = \"alpha = 0.01\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[1].plot(L1trainingscore, label = \"training\")\naxarr[1].plot(L1validationscore, label = \"validation\")\naxarr[1].legend(loc='best')\nlabs = axarr[1].set(title = \"alpha = 0.001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[2].plot(L2trainingscore, label = \"training\")\naxarr[2].plot(L2validationscore, label = \"validation\")\naxarr[2].legend(loc='best')\nlabs = axarr[2].set(title = \"alpha = 0.0001\", xlabel = \"Features\", ylabel = \"Scores\")\naxarr[3].plot(L3trainingscore, label = \"training\")\naxarr[3].plot(L3validationscore, label = \"validation\")\naxarr[3].legend(loc='best')\nlabs = axarr[3].set(title = \"alpha = 0.00001\", xlabel = \"Features\", ylabel = \"Scores\")\n\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.010e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\ayman\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e-02, tolerance: 3.682e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\nKey Takeaway: As Learning rate increases, the LASSO algrotihm declines in the way it performs as we see a shift in paradigm in comparisons of learning rates from 0.01 to 0.00001. Overall, a smaller learning rate helps to improve the data in terms of overfitting as it reduces the declination of clusters."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html",
    "href": "posts/logistic/Gradient-Descent-Blog.html",
    "title": "The Gradient Descent",
    "section": "",
    "text": "https://github.com/aymankhan2003/aymankhan2003.github.io/blob/main/posts/logistic/logistic.py"
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-1-a-case-in-which-gradient-descent-does-not-converge-to-a-minimizer-because-the-learning-rate-is-too-large.",
    "title": "The Gradient Descent",
    "section": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.",
    "text": "Example 1: A case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 0.1\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 5.0, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"alpha = 5.0\", color = \"red\")\n\nLR_X = LogisticRegression()\nLR_X.fit_stochastic(X, y, alpha = 100, max_epochs = 1000, batch_size = 10)\n\nnum_steps = len(LR_X.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_X.loss_history, label = \"alpha = 100.0\", linestyle = \"--\", color = \"green\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the line with moderately higher alpha/learning rate of 5 converges better, while an alpha/learning rate of 100 does not converge as it does not fit, while an alpha/learning rate of 0.1, takes time to converge."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-2-a-case-in-which-the-choice-of-batch-size-influences-how-quickly-the-algorithm-converges.",
    "title": "The Gradient Descent",
    "section": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.",
    "text": "Example 2: A case in which the choice of batch size influences how quickly the algorithm converges.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(1, 1), (-1, -1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\", linestyle = \"--\", color = \"blue\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 30)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"batch_size = 30\", linestyle = \"-\", color = \"yellow\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThis graph shows the lower the batch-size is, the faster it converges, as a batch size of 30 converges faster than a batch size of 50."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "href": "posts/logistic/Gradient-Descent-Blog.html#example-3-if-you-implemented-momentum-a-case-in-which-the-use-of-momentum-significantly-speeds-up-convergence.",
    "title": "The Gradient Descent",
    "section": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.",
    "text": "Example 3: If you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=False)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"no momentum\")\n\nLR_ = LogisticRegression()\nLR_.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10, momentum=True)\n\nnum_steps = len(LR_.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_.loss_history, label = \"momentum\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nThe graph clearly shows the orange line which states momentum converges faster than the blue line which indicates no momentum."
  },
  {
    "objectID": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "href": "posts/logistic/Gradient-Descent-Blog.html#all-lines-together",
    "title": "The Gradient Descent",
    "section": "All Lines Together",
    "text": "All Lines Together\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1), (1, 1)])\n\n#Standard gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\", color = \"skyblue\")\n\n#Stochastic gradient descent\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"stochastic gradient\", color = \"purple\")\n\n#Stochastic gradient descent with momentum\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient (momentum)\", color = \"grey\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nOverall we can see the stochastic gradient with momentum to converge quicker."
  },
  {
    "objectID": "posts/penguins/Penguins-Blog.html",
    "href": "posts/penguins/Penguins-Blog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Reading the Data\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nAfter reading the data, we can see how the data looks like:\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\nVisualizing the DataSet\nThe graphs below shows the relation graph between Culmen Length and Flipper Length, classified by the Species of the Penguins and what Island they are from. From this graph we can see that Gentoo Penguins have bigger flipper lengths, than both Adelie Penguin and Chinstrap Penguins. We can also obtain that Adelie Penguins have Culmen Lengths shorter than Adelie Penguins and Chinstrap Penguins. But it is not possible to classify between Chinstrap Penguins and Adelie Penguins regarding Flipper Length, and also we cannot classify between Culmen Lengths of Gentoo Penguins and Chinstrap Penguins.\nThe relationship between Gentoo Penguins and Adelie Penguins can be classified as linearly seperable but with the addition of Chinstrap Penguins we cannot classify the relationship linearly seperable no more as the data becomes clustered. The first graph helps to also show the Island of Biscoe does not have the species of Chinstrap, while the Island of Dream does not have the species of Gentoo, and lastly the Island of Torgersen does not have both species Gentoo and Chinstrap.\nBecause of the multiple correlations between variables of each species type such as Chinstrap and Adelie regarding Flipper Lengths, and also Gentoo and Chinstrap regarding Culmen Lengths, our model would find it hard to identify Chinstrap from Adelie based on Flipper Lengths itself, and also Gentoo from Chinstrap based on Culmen Lengths.\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species', col = 'Island')\n\n<seaborn.axisgrid.FacetGrid at 0x2197a49cbe0>\n\n\n\n\n\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species')\n\n<seaborn.axisgrid.FacetGrid at 0x21979f27fa0>\n\n\n\n\n\nThe table below shows the Average Flipper Lengths, and Body Mass of the Species based on their Sex. We can see from the table that Gentoo Penguins have a greater body mass then both Chinstrap and Adelie Penguins, while Male Adelie Penguins have greater body mass then Chinstrap Penguins. The table also shows that Gentoo Penguins also have greater Flipper Lengths then the other two species while Male Adelie Penguins have greater flipper lengths then the female Chinstrap Penguins.\nUsing this table we can say that our model can classify between Gentoo Penguins from the rest but classifying the other two penguins using these two variables can be hard as they intersect in some instances. It canc lassify, but it will have to keep in track for each instance respective to it’s Sex.\n\ntrain.groupby(['Species', 'Sex'])[['Flipper Length (mm)', 'Body Mass (g)']].mean()\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      Species\n      Sex\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      187.719298\n      3337.280702\n    \n    \n      MALE\n      192.690909\n      4020.454545\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      191.551724\n      3514.655172\n    \n    \n      MALE\n      199.666667\n      3936.111111\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      FEMALE\n      212.928571\n      4677.976190\n    \n    \n      MALE\n      221.462963\n      5502.314815\n    \n  \n\n\n\n\n\n\nCleaning the DataSet\nDropping irrelavent variables that will not help with our model, and preparing our data for training:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nFeature Engineering\nThe code below takes in the given qualitative columns and quatitave columns and uses the variables to calculate the Logistic Regression scores for all combinations. Then it prints out the columns/variables with the best score, and the output below shows that the columns ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’ are the best for the model to classify data as it outputs a score very close to 1.0.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nLR = LogisticRegression()\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    \n    if score > best_score:\n        best_column = cols\n        best_score = score\n        best_model = LR\n    \nprint(best_column, best_score)\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.99609375\n\n\nWe can visualize the columns of our training data set, where 1 refers to the penguin belonging in that Island, and 0 states it doesn’t:\n\nX_train[cols]\n\n\n\n\n\n  \n    \n      \n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Flipper Length (mm)\n      Body Mass (g)\n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      215.0\n      5000.0\n    \n    \n      2\n      0\n      0\n      1\n      202.0\n      3875.0\n    \n    \n      3\n      0\n      1\n      0\n      185.0\n      3650.0\n    \n    \n      4\n      0\n      1\n      0\n      193.0\n      3800.0\n    \n    \n      5\n      0\n      1\n      0\n      178.0\n      2900.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      0\n      1\n      0\n      190.0\n      3900.0\n    \n    \n      270\n      1\n      0\n      0\n      211.0\n      4800.0\n    \n    \n      271\n      0\n      0\n      1\n      187.0\n      3150.0\n    \n    \n      272\n      1\n      0\n      0\n      224.0\n      5350.0\n    \n    \n      273\n      1\n      0\n      0\n      210.0\n      4600.0\n    \n  \n\n256 rows × 5 columns\n\n\n\nPreparing our data for future testing:\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\n\nFunction to plot our results\nThe code below creates a function that takes in the given model that we provide, and uses the testing/training datasets to display it in a nice region plot, which can help us visualize the classification properly.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n    \n      plt.title(title)\n        \n      plt.tight_layout()\n\n\n\nTesting/Training the Model\nWe can see from the result that the training dataset recieved about 97% accuracy which isn’t bad at all, showing only overfitting between three-four penguins so overall our model did a good job solely based on the training. We can see that it does not overfit between Adelie and Gentoo, but does a very little bit for Gentoo and Chinstrap which is totally fine as we are able to see a proper classification of the penguins solely on the training.\nNow we can see that the testing also obtained about 97% accuracy which also isn’t bad at all, as it does not overfit much, as it only intersects for about one penguin which is acceptable hence our model is able to classify the penguins properly achieving about 97% accuracy.\n\nX_train[\"Island_Biscoe_No\"] = X_train[[\"Island_Dream\", \"Island_Torgersen\"]].sum(axis=1)\nX_test[\"Island_Biscoe_No\"] = X_test[[\"Island_Dream\", \"Island_Torgersen\"]].sum(axis=1)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Biscoe_No']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nscore_train = LR.score(X_train[cols], y_train)\nscore_test = LR.score(X_test[cols], y_test)\n\nprint(\"Score for training: \" + str(score_train))\nplot_regions(LR, X_train[cols], y_train, title = 'Training')\n\nprint(\"Score for testing: \" + str(score_test))\nplot_regions(LR, X_test[cols], y_test, title = 'Testing')\n\nScore for training: 0.9765625\nScore for testing: 0.9705882352941176"
  },
  {
    "objectID": "posts/perceptron/Perception Algorithm.html",
    "href": "posts/perceptron/Perception Algorithm.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Perceptron Update\n\ndef fit(self, X, y, max_steps):\n    n_samples, n_features = X.shape\n    self.w = np.random.rand(n_features+1, )\n        \n    for _ in range(self.max_steps):\n        j = np.random.randint(X.shape[0])\n        xi = np.append(X[j], 1)\n        y_hat = np.dot(xi, self.w)\n        yi = 2*y[j] - 1\n        self.w += (yi * (np.dot(xi, self.w)) < 0) * yi*xi\n                    \n        accuracy = self.score(X, y)\n        self.history.append(accuracy)\n        if self.history[_] == 1:\n            break \n\nThe major element in this update of the data plots is the fit function from the perceptron class. In order to introduce an extra bias with the value 1 to the input data, we start by randomly integrating the weights vector and adding one to n features. Our loop, where j defines a random index is taken from the number of samples in the input data “X,” is carried out with an iteration of max steps. X[j] appends 1 alongside to make the vector consistent with the weights vector. The anticipated output, denoted by the dot product of x and the weights, is denoted by y_hat, while the actual output, denoted by yi, transforms Y[j] to either 1 or -1. The weights are then updated using the loss function, and the accuracy of the perceptron point is then monitored by looking at the score history. We terminate when the history adding reaches its final iteration, which is 1, which indicates that all samples have been correctly classified.\n\n\nExample 1\n\nUsing 2d data we check if the data is linearly separable then the perceptron algorithm converges to weight vector w^tilde describing a separating line:\n\nfrom perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nExample 2\n\nUsing 2d data again, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w^tilde, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy:\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0.5, 0.5), (0.5, 0.5)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()\n\n\n\n\n\n\n\nExample 3\n\nThe perceptron algorithm is also thought to be able to work in more than 2 dimensions, we can check if it actually does:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\n\n\n\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\n\n\n\nTime Complexity of Perceptron Algorithm Update\n\nThe perceptron algorithm update has a runtime complexity of O(p), where p is the number of features. This is due to the dot product operation, which requires O(p) time, between the input vector xi and the weight vector self.w. The remaining operations are independent of the number of features and have constant time complexity."
  },
  {
    "objectID": "posts/project/ProjectBlogPost.html",
    "href": "posts/project/ProjectBlogPost.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "In the world of soccer, the market value serves as one of the main sources for generating revenue for soccer clubs. Over the last few years, the money spent on the market value has sky-rocketed with FIFA, the governing body in soccer, revealing that soccer clubs spent $7.35 billion in player acquisitions just in 2019. In this high stake environment, with so much money involved, it is important for clubs to accurately assess the market value of players before submitting a bid to buy a player. However, the traditional crowd-sourced approaches have been questioned for their inconsistency and susceptibility to bias. Recognizing this challenge, we aim to build an alternative to the conventional crowd-sourced approach to determining a player’s market value, using the power of machine learning algorithms. Our project not only targets determining a player’s market value, but also addresses a more complex issue in soccer finance: the ‘transfer fee’. The ‘transfer fee’—the total cost a club pays for a player—extends beyond the player’s market value, incorporating additional monetary obligations imposed by selling clubs. In the project, we have developed a machine learning model that relies on linear regression and random forest algorithms to estimate the transfer fee. This model has the potential to help clubs and transfer agents for strategic financial planning and reducing the risk of overpayment."
  },
  {
    "objectID": "posts/project/ProjectBlogPost.html#data",
    "href": "posts/project/ProjectBlogPost.html#data",
    "title": "Project Blog Post",
    "section": "Data",
    "text": "Data\nOur project has been made possible through the compilation of data from three distinct websites, which served as primary sources for both training and testing our models.\nSofifa.com: This source primarily provides us with player ratings and potential scores and other attributes such as shooting scores, passing scores etc. from the popular video game series, FIFA, developed by EA Sports. The ratings are derived from the efforts of over 6,000 FIFA Data Reviewers, also known as Talent Scouts. (Murphy, 2021). This collaborative work helps ensuring that player ratings are continuously updated and reflective of their real-time performance .The site’s updates are carried out weekly, thereby providing us with relevant data.\nTransfermarkt.com: This platform is our primary source for club data, serving as a resource for statistics related to clubs that individual players play for. Transfermarkt is a popular entity in the soccer world, known for its estimation of player market values. However, these values have been subject to criticism in the past, as noted in this New York Times article (Smith,2021). Consequently, for the purpose of our project, we’ve chosen to focus solely on factual club data. This includes league positions over the years, player rosters, amount of money spent in the market value, goals scored, and other quantifiable statistics that are a reliable record of the club’s performance.\nKaggle.com: Originally, our strategy was to compile transfer data from 2018, coinciding with the final update to the data in FIFA 2017, just prior to the opening of the 2018 market value—a period of three months during which soccer clubs are permitted to trade players. However, given the limited number of player trades that occur each year and the resulting time-intensive nature of annual data scraping, we found a more efficient solution. We found a large dataset on Kaggle, containing transfer data from European leagues from 1992 to 2021. This data was published by a Kaggle user, Bhavik Chadna, and was initially scraped from TransferMarkt.com. In a significant enhancement to our methodology, we segmented this dataset by league and by year. This allowed us to merge data for each league, for each year, with corresponding player data from the respective FIFA years, as obtained from Sofifa.com. The final stage of our data processing involved aggregating all our segmented datasets into a singular, comprehensive CSV file. The resultant dataset comprises transfer data from 2015 to 2021, from prominent leagues including La Liga (Spain), Premier League (England), Ligue 1 (France), Serie A (Italy), and Bundesliga (Germany). This was subsequently aligned with player data from the corresponding years from Sofifa. In sum, our project utilizes data from Kaggle, effectively harmonizing it with data from Sofifa and Transfermarkt. This integrative approach ensures that our model is trained on the most comprehensive and accurate data possible.\nUpon gathering and filtering the data, we finalized with two distinct datasets for each of our project models.\nMarket Value Model: This model is centered around the dataset derived from Sofifa’s FIFA 2017 statistics. This dataset comprises a total of 5700 observations, each comprising 59 features. Looking back, we recognize that the volume of observations could have been significantly expanded by integrating data from multiple years of FIFA. However, due to time constraints and satisfactory performance of the model with the current dataset, we concluded that the expansion was not required.\nTransfer Fee Model: Our second model, focusing on transfer fees, was trained and tested on a dataset, with 619 observations and 78 distinct features. Although the dataset encompasses transfer records spanning from 2015 to 2021, the relatively small number of observations underlines an important aspect of the soccer market value: only a limited number of players are traded each year.\nOur sources hold a good ethical standard mostly because of the nature of sports data, including soccer, which is generally made public. Soccer clubs are usually open about player transfers, making our sources such as Transfermarkt essentially collection points for this readily available information."
  },
  {
    "objectID": "posts/project/ProjectBlogPost.html#approach",
    "href": "posts/project/ProjectBlogPost.html#approach",
    "title": "Project Blog Post",
    "section": "Approach",
    "text": "Approach\nNOTE: For much more on the specifics + useful resources that helped us w/ this portion of project, refer to: Project.ipynb file in Github Repository\nMarket Value Model:\nOur imported data was fairly clean and we had plenty of samples and features (5700, 59). When preparing our data for model training, we dropped categorical features like name and field position and any samples with features containing N/A values. Also, we dropped release clause and wage because these features seem to be calculated by Fifa with the same formula that is used for market value so they were biased predictors. After all this, we got down to 5010 samples and 57 features. We used 20% of this data as a holdout test set and randomized our test_train_split because our original data was ordered by the player’s overall rating and we didn’t want the model to train on “good players” and test on “bad players” at the tail end of our dataset. We standardized our data using Sklearn’s StandardScaler() so the coefficients of our linear predictors could be more easily interpreted. Our target vector was the “value” column of our data and the rest of the features comprised our feature matrix.\nThereafter, we performed feature selection. We combined univariate feature selection approaches like mutual information and ANOVA with recursive feature elimination (w/ cross-validation) using Lasso Linear Regression to select the “most important” predictive features.\nWe trained our models on the standout selected features from our univariate + RFE approach (Age and overall rating seemed to be the most important ones). We used a LASSO model and a RandomForestRegressor model to observe the difference between linear and non-linear patterns.\nTo examine model success, we computed the score (coefficient of determination) on the holdout set and used cross-validation as well to ensure that the ranndomness of the train test split wasn’t giving us optimistic or pessimistic results on the holdout set.\nWe also plotted learning curves, actual vs. predicted, and residuals to visually aid in examining our model’s underfitting/overfitting tendencies.\nTransfer Fee Model:\nOur imported data had 708 samples and 78 features but it was not very clean. We had to subset the data for modeling to just field players (excluded goalkeepers) because goalkeepers had N/A values for many of the non-goalkeeping related features. We got down to 414 samples and 73 features (dropped categorical ones) for our field player data that we would use for modeling. We used 20% of this as a houldout test set and randomized our test_train_split. We standardized our data using Sklearn’s StandardScaler() so the coefficients of our linear predictors could be more easily interpreted. Our target vector was the “fee_cleaned” column of our data (which we turned into millions) and the rest of the features comprised our feature matrix.\nBased on the literature from Ian G. McHale, Benjamin Holmes(https://www.sciencedirect.com/science/article/pii/S0377221722005082#bib0029), we engineered two features with high predictive power: avg. price paid by selling club and avg. price paid by buying club.\nWe performed the same process for feature selection as described for the market value model. The most notable and intuitive of these features were: ‘fee_cleaned_buyer_avg’(avg. price paid by buying club), ‘fee_cleaned_seller_avg’(avg. price paid by selling club), ‘value_eur’(market value). We also used age and potential as features because in the literature these features were deemed important by the author’s models.\nOnce again, we used a LASSO and Random Forest Regressor to obseve differences between the linear and ensemble approaches.\nGiven that this was a more complicated task than predicting market value, we tuned hyperparameters to squeeze out the best possible model performance for each model using nested cross-validation as we also wanted to use cross-validation to assess the success of the model with the best hyperparameters without optimistic biases.\nWe also computed the accuracy of the model on the holdout-set and used cross-validation, though neither of these measures were robust performance estimates as we had hoped them to be as is further explained in the results section.\nWe also plotted learning curves, actual vs. predicted, and residuals to visually aid in examining our model’s underfitting/overfitting tendencies.\nWe also performed an audit for bias by nationality. We picked 4 relatively well-represented countries (enough/similar amount of samples) in our dataset that are associated with great football players: England, France, Italy, Germany. First, we examined the disparities in “actual” transfer fee values for these countries in the dataset. We then used our “best” model for predictions to examine the disparities in “predicted” transfer fee values for these countries using both mean and median since outliers (though an integral and non-trivial aspect of the football market) made some predictions for some countries appear worse than others. We determined a threshold for “good” predictions and calculated the proportion of good predictions by country to further assess bias possibilities."
  },
  {
    "objectID": "posts/project/ProjectBlogPost.html#implementing-a-small-user-interface-for-our-model",
    "href": "posts/project/ProjectBlogPost.html#implementing-a-small-user-interface-for-our-model",
    "title": "Project Blog Post",
    "section": "Implementing a Small User Interface for our Model",
    "text": "Implementing a Small User Interface for our Model\nAfter receiving nearly 90% accuracy for our market value mode, we decided to build a small user interface for our model that, upon receiving a player’s name via user input, outputs the player’s predicted market value. We also integrated our market value model into the UI, which calculates how much a team would have to pay for a player.\nWe started by creating separate Python files for each of our market value codes and market value codes, which we then used as modules in our main Flask backend file. After we finished our modules, we used conditionals to incorporate them into our flask file, and each module would run upon the retrieval of each user’s input. A minor disadvantage of our program is that, because the model divides the data set into training and testing, not all players are available; thus, it may take a few input tries to obtain the market value of a specific player (for example, if the user enters “Robert Lewandowski” and he is in the testing data set, his value will be displayed; otherwise, the user may need to enter his name a couple of times until he is in the testing data set). We created our main front end designs using HTML, CSS, and JavaScript in one HTML file. We created our main front end designs in one HTML file by combining HTML, CSS, and JavaScript. The frontend itself was not complicated, as it only required sufficient time for page design, such as the colors, margins, and positions of each attribute visible in the UI.\nInstructions for using the UI: - Navigate to the Github repository. - Download all of the files because each one is important to the model and app. - Then open the Python files and navigate to the directory where you saved them on your computer. - Then, within the Python notebook, run app.py, then navigate to the command shell and run the following command: python app.py - You can experiment with the UI by clicking on the link provided in the command shell. Unfortunately, because this is a - built-in Flask webpage, the link cannot be shared; thus, this is the only way to experiment with the app. - When running the Market Value Model UI, player names are formatted like “R. Lewandowski,” where the first name is abbreviated to just the first letter with a period and the last name, whereas for some players with no big last name, such as “’Neymar Jr,” the first name is spelled out and the last name is kept as is. Whereas the player name is formatted with their full name for the Transfer Fee Model, such as “Robert Lewandowski”.\n\n\n\nUI Page Cover.jpg\n\n\nDisclaimer: The UI currently does not look like this, as Flask cannot render the background image. We edited the design to work in accordance with Flask, but the goal was for it to look like the image above which we did code but would only display as we opened the HTML file without the backend code. The original is similar, but with a white background instead of the background image which was designed by Anweshan."
  },
  {
    "objectID": "posts/project/ProjectBlogPost.html#accuracy-measures",
    "href": "posts/project/ProjectBlogPost.html#accuracy-measures",
    "title": "Project Blog Post",
    "section": "Accuracy Measures:",
    "text": "Accuracy Measures:\n\nSummary:\nLASSO: About 75 % accuracy on both holdout & cross-validation approach\nRFR: About 95 % accuracy on both holdout & cross-validation approach\n\n\nLearning Curves From Project File:\n\n\n\nimage.png\n\n\n\n\nActual vs. Predicted + Residual Plot From Project File:\n\n\n\nimage.png\n\n\nTransfer Fee Model:\n\n\nSummary:\nNOTE: Scores fluctuate from about the 60-70 % range based on the train_test_split. This could be accounted for by refitting and rescoring through various train_test_split loops, but is very time consuming.\nLASSO: About 65 % accuracy using cross-validation; About 85 % on holdout set\nRFR: About 70 % accuracy using cross-validation approach; About\n\n\nLearning Curves From Project File:\n #### Take-away:\n\n\nActual vs. Predicted + Residual Plot From Project File:\n NOTE: The residual plots have different y_axis limits to account for data-specific outliers, so double checking the axis to interpret the residual plot instead of doing it spatially is important. #### Take-away: Both models are relatively accurate when predicting smaller transfer fees, but the higher the transfer fee goes, the less accurate the predictions are. Some outliers throw the models off."
  },
  {
    "objectID": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "href": "posts/unsupervisedlearning/UnsupervisedLearning.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\n\na_1 = np.random.randint(1, 3, (5, 3))\na_2 = np.random.randint(1, 3, (3, 7))\n\nA = a_1 @ a_2 + 0.1*np.random.randn(5, 7)\nA\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\n\n\n\nplt.imshow(A, cmap = \"Greys\")\na = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nU, sigma, V = np.linalg.svd(A)\n\n\n# create the D matrix in the SVD\nD = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\nD[:min(A.shape),:min(A.shape)] = np.diag(sigma)        # singular values on the main diagonal\nD\n\narray([[47.92787806,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  1.59525638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.35772778,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.23426796,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.15684841,\n         0.        ,  0.        ]])\n\n\n\nU @ D @ V # == A up to numerical precision\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\n\nk = 2\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\n\n\nA_ = U_ @ D_ @ V_\n\n\n\n\n\ndef compare_images(A, A_):\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n\ncompare_images(A, A_)\n\n\n\n\n\nk = 1\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\nA_ = U_ @ D_ @ V_\ncompare_images(A, A_)\n\n\n\n\nThe images almost look visually close and similar\n\n\n\n\n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://wallpapers.com/images/hd/cute-spongebob-playing-soccer-cd7tg9yci4brk87w.jpg\"\n\nimg = read_image(url)\n\n\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\ngrey_img.shape\n\n(1080, 1920)\n\n\n\n\n\n\ndef svd_reconstruct(image, k):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    A = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A\n\n\n\n\n\n\n\n\n\n\ndef svd_experiment(image):\n    # Original image dimensions\n    m, n = image.shape\n    \n    # Total number of pixels in the original image\n    total_pixels = m * n\n    \n    rows = 4\n    cols = 4\n    fig, axarr = plt.subplots(rows, cols, figsize=(20, 10))\n    \n    k_values = [5 * i for i in range(1, 17)]\n    \n    for i, k in enumerate(k_values):\n        reconstructed_image = svd_reconstruct(image, k)\n        \n        # Calculate the storage required for the reconstructed image\n        reconstructed_pixels = k * (m + n + k) \n            \n        # Display the reconstructed image for visual comparison\n        subplot_row = i // cols\n        subplot_col = i % cols\n        subplot_ax = axarr[subplot_row, subplot_col]\n        \n        subplot_ax.imshow(reconstructed_image, cmap='Greys')\n        title = f'{k} components %storage = {round((reconstructed_pixels / total_pixels)*100, 1)}'\n        subplot_ax.set_title(title)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nsvd_experiment(grey_img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef svd_reconstruct(image, cf):\n    # Perform singular value decomposition\n    U, sigma, V = np.linalg.svd(image)\n    \n    # Truncate singular values\n    D = np.zeros_like(image, dtype=float)\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)\n    \n    m, n = image.shape\n    #Solving compression factor equation in terms of k\n    k = (-m + n + np.sqrt(m**2 + 2*m*n + n**2 + 4*cf*m*n)/2)\n    k = np.floor(k)\n    k = k.astype(int)\n    A_ = U[:, :k] @ D[:k, :k] @ V[:k, :]\n    \n    return A_\n\n\nnewimage = svd_reconstruct(grey_img, 20)\ncompare_images(grey_img, newimage)"
  }
]